# GPT3:Language Models are Few-shot Learners
## few-shot:
the same as human, need samples to help model to learn the model. But different as the front one which needs a quite huge amount of samples, this one only need a few
## Abstract
train GPT-3 with 175 billion parameters, 10x more than any previouse nnon-sparse language model, without any gradient update or fine-tuning<br>
can generate samples of news articles

## Introduction
+ recent years have featured a trend towards pre-trained language representations in NLP systems
+ have problem:
++ a need for task-specific database and task-specific fine-tuning
++ when models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distribution, the fine-tuned model maybe not satisfied to the data which not appears in the database<br>
big model is easy to overfitting on the small database, so a good fine-tuning result does not mean a good generalization.
+ how to solve it:
++ figure 1.1<br>
language model meta-learning(few shot/sero shot)
++ figure 1.2<br>
meta-learning（in-context learning）,don't need to do any refresh
